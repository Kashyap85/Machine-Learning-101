{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tkinter as tk\n",
    "window = tk.Tk()\n",
    "window.title('Regression!!')\n",
    "label = tk.Label(window, text = \"Hello World!\").pack()\n",
    "l1 = tk.Label(window, text=\"edureka!\").pack()\n",
    "#window.geometry('350x200')\n",
    "#l1.grid(column=0,row=0)\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin=LinearRegression()\n",
    "lin.fit(x_train,y_train)\n",
    "y_pred_linreg=lin.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ARDRegression\n",
    "#ARD Regression\n",
    "clf=linear_model.ARDRegression() \n",
    "clf.fit(x_train,y_train)\n",
    "y_pred=clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Ridge Regression\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "reg=linear_model.BayesianRidge()\n",
    "reg.fit(x_train,y_train)\n",
    "y_pred_bayridge=reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ElasticNet Regession\n",
    "from sklearn.linear_model import ElasticNet\n",
    "model=ElasticNet()\n",
    "model.fit(x_train,y_train)\n",
    "y_pred_elastic=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lars Regression\n",
    "from sklearn.linear_model import Lars\n",
    "reg=Lars()\n",
    "reg.fit(x_train,y_train)\n",
    "y_pred_lars=reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso Regression\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso=Lasso(alpha=100)\n",
    "lasso.fit(x_train,y_train)\n",
    "y_pred_lasso=lasso.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression---> can be used for two features only\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "reg=LogisticRegression(random_state=0).fit(x_train,y_train)\n",
    "y_pred_logreg=reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA + Logistic Regression\n",
    "import sklearn.linear_model as sklm ##Includes Logistic Regression, which will be tested for predictive capability\n",
    "import sklearn.decomposition as skdc ##Includes Principal Component Analysis, a method of dimensionality reduction\n",
    "import sklearn.pipeline as skpl ##Convenient module for calculating PCs and using them in logistic regression\n",
    "pca = skdc.PCA() #empty model space\n",
    "pcafit = pca.fit_transform(x,y) ##apply dimensionality reduction to X\n",
    "var_explained = pca.explained_variance_ratio_ #ratio of variance each PC explains\n",
    "print(pd.Series(var_explained))\n",
    "###Since 29 components aren't necessary, the last 20 PCs will be disregarded \n",
    "###since they explain less than.01 of the variance\n",
    "##indeed,the first 10 PCs explain 95% of the variance\n",
    "pca = skdc.PCA(n_components = 10) #only include first 10 components\n",
    "logreg = sklm.LogisticRegression()#empty model space\n",
    "pipeline = skpl.Pipeline([('pca', pca), ('logistic', logreg)]) #create pipeline from pca to logregression space\n",
    "fit = pipeline.fit(x_train, y_train) #fit model\n",
    "y_pred_pcalog = pipeline.predict(x_test) #test model with left out value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polynommial Regression->Linear Regression with polynomial features\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly=PolynomialFeatures(degree=3)\n",
    "x=poly.fit_transform(x)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.1,random_state=0)\n",
    "print(x_train,y_train)\n",
    "poly.fit(x_train,y_train)\n",
    "lin2=LinearRegression()\n",
    "lin2.fit(x_train,y_train)\n",
    "y_pred_polyreg=lin2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor=RandomForestRegressor(n_estimators=100,random_state=0)\n",
    "regressor.fit(x_train,y_train)\n",
    "y_pred_randf=regressor.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regr=DecisionTreeRegressor(max_depth=4)\n",
    "regr.fit(x_train,y_train)\n",
    "y_pred_dectree=regr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "regr=AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),n_estimators=291)\n",
    "regr.fit(x_train,y_train)\n",
    "y_pred_adaboodst=regr.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "rr=Ridge(alpha=50)\n",
    "rr.fit(x_train,y_train)\n",
    "y_pred_ridge=rr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "clf=SGDRegressor(max_iter=1000,tol=1e-3)\n",
    "clf.fit(x,y)\n",
    "y_pred_sgd=clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVR(kernel='rbf')\n",
    "clf.fit(x,y)\n",
    "y_pred_svr=clf.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
